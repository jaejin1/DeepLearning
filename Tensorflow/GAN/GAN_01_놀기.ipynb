{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "n_hidden = 256\n",
    "n_hidden2 = 512\n",
    "n_input = 28 * 28\n",
    "n_noise = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nD_W2 = tf.Variable(tf.random_normal([n_hidden,n_noise], stddev = 0.01))\\nD_b2 = tf.Variable(tf.zeros([n_noise]))\\n\\nD_W3 = tf.Variable(tf.random_normal([n_noise, 1], stddev=0.01))\\nD_b3 = tf.Variable(tf.zeros([1]))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "# 생성기\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "'''\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_hidden2], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_hidden2]))\n",
    "\n",
    "G_W3 = tf.Variable(tf.random_normal([n_hidden2, n_input], stddev=0.01))\n",
    "G_b3 = tf.Variable(tf.zeros([n_input]))\n",
    "'''\n",
    "\n",
    "# 판별기\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input,n_hidden], stddev = 0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "'''\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden,n_noise], stddev = 0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([n_noise]))\n",
    "\n",
    "D_W3 = tf.Variable(tf.random_normal([n_noise, 1], stddev=0.01))\n",
    "D_b3 = tf.Variable(tf.zeros([1]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성기 신경망\n",
    "\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    #output = tf.nn.sigmoid(tf.matmul(hidden2,G_W3) + G_b3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 판별기 신경망\n",
    "\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    #output = tf.nn.sigmoid(tf.matmul(hidden2, D_W3) + D_b3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 랜덤한 노이즈만들기\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(z)\n",
    "D_gene = discriminator(G)\n",
    "\n",
    "D_real = discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1-D_gene))\n",
    "loss_G = tf.reduce_mean(tf.log(1-D_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,\n",
    "                                                         var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(loss_G,\n",
    "                                                         var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -0.4334 G loss: -0.1247\n",
      "Epoch: 0001 D loss: -0.1769 G loss: -0.05948\n",
      "Epoch: 0002 D loss: -0.07795 G loss: -0.0337\n",
      "Epoch: 0003 D loss: -0.3234 G loss: -0.1262\n",
      "Epoch: 0004 D loss: -0.2211 G loss: -0.09372\n",
      "Epoch: 0005 D loss: -0.2062 G loss: -0.1184\n",
      "Epoch: 0006 D loss: -0.1451 G loss: -0.08401\n",
      "Epoch: 0007 D loss: -0.2989 G loss: -0.1254\n",
      "Epoch: 0008 D loss: -0.2087 G loss: -0.07888\n",
      "Epoch: 0009 D loss: -0.1123 G loss: -0.042\n",
      "Epoch: 0010 D loss: -0.6338 G loss: -0.1998\n",
      "Epoch: 0011 D loss: -0.3141 G loss: -0.08525\n",
      "Epoch: 0012 D loss: -0.2316 G loss: -0.07126\n",
      "Epoch: 0013 D loss: -0.3623 G loss: -0.1676\n",
      "Epoch: 0014 D loss: -0.08978 G loss: -0.04069\n",
      "Epoch: 0015 D loss: -0.3496 G loss: -0.1397\n",
      "Epoch: 0016 D loss: -0.2509 G loss: -0.09402\n",
      "Epoch: 0017 D loss: -0.1767 G loss: -0.05096\n",
      "Epoch: 0018 D loss: -0.1677 G loss: -0.05724\n",
      "Epoch: 0019 D loss: -0.1513 G loss: -0.06243\n",
      "Epoch: 0020 D loss: -0.2277 G loss: -0.07291\n",
      "Epoch: 0021 D loss: -0.07804 G loss: -0.01962\n",
      "Epoch: 0022 D loss: -0.2144 G loss: -0.0975\n",
      "Epoch: 0023 D loss: -0.1329 G loss: -0.04499\n",
      "Epoch: 0024 D loss: -0.1115 G loss: -0.0525\n",
      "Epoch: 0025 D loss: -0.1411 G loss: -0.06963\n",
      "Epoch: 0026 D loss: -0.1142 G loss: -0.05376\n",
      "Epoch: 0027 D loss: -0.1803 G loss: -0.08085\n",
      "Epoch: 0028 D loss: -0.2082 G loss: -0.1226\n",
      "Epoch: 0029 D loss: -0.1722 G loss: -0.06761\n",
      "Epoch: 0030 D loss: -0.2151 G loss: -0.05584\n",
      "Epoch: 0031 D loss: -0.1798 G loss: -0.07771\n",
      "Epoch: 0032 D loss: -0.1452 G loss: -0.05817\n",
      "Epoch: 0033 D loss: -0.2055 G loss: -0.1093\n",
      "Epoch: 0034 D loss: -0.2696 G loss: -0.1183\n",
      "Epoch: 0035 D loss: -0.3148 G loss: -0.1245\n",
      "Epoch: 0036 D loss: -0.2438 G loss: -0.08436\n",
      "Epoch: 0037 D loss: -0.2442 G loss: -0.1264\n",
      "Epoch: 0038 D loss: -0.287 G loss: -0.07736\n",
      "Epoch: 0039 D loss: -0.2292 G loss: -0.1101\n",
      "Epoch: 0040 D loss: -0.3799 G loss: -0.2142\n",
      "Epoch: 0041 D loss: -0.1083 G loss: -0.0591\n",
      "Epoch: 0042 D loss: -0.1934 G loss: -0.1133\n",
      "Epoch: 0043 D loss: -0.1426 G loss: -0.0835\n",
      "Epoch: 0044 D loss: -0.169 G loss: -0.09917\n",
      "Epoch: 0045 D loss: -0.18 G loss: -0.1037\n",
      "Epoch: 0046 D loss: -0.07956 G loss: -0.03721\n",
      "Epoch: 0047 D loss: -0.05854 G loss: -0.0215\n",
      "Epoch: 0048 D loss: -0.4327 G loss: -0.1695\n",
      "Epoch: 0049 D loss: -0.2801 G loss: -0.1217\n",
      "Epoch: 0050 D loss: -0.2887 G loss: -0.1712\n",
      "Epoch: 0051 D loss: -0.1399 G loss: -0.06988\n",
      "Epoch: 0052 D loss: -0.2637 G loss: -0.1257\n",
      "Epoch: 0053 D loss: -0.5538 G loss: -0.2121\n",
      "Epoch: 0054 D loss: -0.06307 G loss: -0.022\n",
      "Epoch: 0055 D loss: -0.03819 G loss: -0.01198\n",
      "Epoch: 0056 D loss: -0.3998 G loss: -0.1612\n",
      "Epoch: 0057 D loss: -0.526 G loss: -0.2214\n",
      "Epoch: 0058 D loss: -0.06123 G loss: -0.02419\n",
      "Epoch: 0059 D loss: -0.01874 G loss: -0.01098\n",
      "Epoch: 0060 D loss: -0.2505 G loss: -0.1116\n",
      "Epoch: 0061 D loss: -0.3565 G loss: -0.1257\n",
      "Epoch: 0062 D loss: -0.4335 G loss: -0.1478\n",
      "Epoch: 0063 D loss: -0.349 G loss: -0.07492\n",
      "Epoch: 0064 D loss: -0.4388 G loss: -0.2595\n",
      "Epoch: 0065 D loss: -0.3488 G loss: -0.1646\n",
      "Epoch: 0066 D loss: -0.4402 G loss: -0.2019\n",
      "Epoch: 0067 D loss: -0.329 G loss: -0.1647\n",
      "Epoch: 0068 D loss: -0.1263 G loss: -0.04297\n",
      "Epoch: 0069 D loss: -0.04142 G loss: -0.01763\n",
      "Epoch: 0070 D loss: -0.04344 G loss: -0.01479\n",
      "Epoch: 0071 D loss: -0.01239 G loss: -0.002782\n",
      "Epoch: 0072 D loss: -0.3087 G loss: -0.1203\n",
      "Epoch: 0073 D loss: -0.3812 G loss: -0.1364\n",
      "Epoch: 0074 D loss: -0.4563 G loss: -0.1757\n",
      "Epoch: 0075 D loss: -0.3082 G loss: -0.1745\n",
      "Epoch: 0076 D loss: -0.09089 G loss: -0.05964\n",
      "Epoch: 0077 D loss: -0.03972 G loss: -0.01722\n",
      "Epoch: 0078 D loss: -0.4833 G loss: -0.1467\n",
      "Epoch: 0079 D loss: -0.4277 G loss: -0.1897\n",
      "Epoch: 0080 D loss: -0.1183 G loss: -0.05774\n",
      "Epoch: 0081 D loss: -0.4044 G loss: -0.1347\n",
      "Epoch: 0082 D loss: -0.4693 G loss: -0.2474\n",
      "Epoch: 0083 D loss: -0.4142 G loss: -0.1912\n",
      "Epoch: 0084 D loss: -0.3769 G loss: -0.2133\n",
      "Epoch: 0085 D loss: -0.3059 G loss: -0.1135\n",
      "Epoch: 0086 D loss: -0.2308 G loss: -0.1099\n",
      "Epoch: 0087 D loss: -0.09594 G loss: -0.05498\n",
      "Epoch: 0088 D loss: -0.4211 G loss: -0.136\n",
      "Epoch: 0089 D loss: -0.3572 G loss: -0.172\n",
      "Epoch: 0090 D loss: -0.4115 G loss: -0.1632\n",
      "Epoch: 0091 D loss: -0.5268 G loss: -0.2101\n",
      "Epoch: 0092 D loss: -0.3396 G loss: -0.2134\n",
      "Epoch: 0093 D loss: -0.2835 G loss: -0.1561\n",
      "Epoch: 0094 D loss: -1.747 G loss: -0.8685\n",
      "Epoch: 0095 D loss: -0.1082 G loss: -0.04686\n",
      "Epoch: 0096 D loss: -0.02941 G loss: -0.01126\n",
      "Epoch: 0097 D loss: -0.01329 G loss: -0.006817\n",
      "Epoch: 0098 D loss: -0.01086 G loss: -0.004018\n",
      "Epoch: 0099 D loss: -0.005674 G loss: -0.002006\n",
      "끝!\n"
     ]
    }
   ],
   "source": [
    "### 모델 학습\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D , loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        # 판별기와 생성기 신경망을 각각 학습시킨다.\n",
    "        _, loss_val_D = sess.run([train_D, loss_D], \n",
    "                                 feed_dict = {X: batch_xs, z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], \n",
    "                                 feed_dict = {z: noise})\n",
    "        \n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "         'D loss: {:.4}'.format(loss_val_D),\n",
    "         'G loss: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    \n",
    "    ### 학습이 이뤄지는거 확인하기위해서 이미지 생성\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict = {z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(1, sample_size , figsize=(sample_size, 1))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "            \n",
    "        plt.savefig('samples_test/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('끝!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-6700090ffbe5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-6700090ffbe5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1. 신경망 추가하기\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. 신경망 추가하기\n",
    "2. 데이터 바꿔보기 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
